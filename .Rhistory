names(test)
test <- test[ , colSums(is.na(test)) != nrow(test)]
train <- train[ ,colSums(is.na(train)) != nrow(train)]
names(t)
test <- read.csv("pml-testing.csv", header = TRUE)
train <- read.csv("pml-training.csv", header = TRUE)
test <- test[ , colSums(is.na(test)) != nrow(test)]
train <- train[ ,colSums(is.na(train)) != nrow(train)]
names(test)
names(train)
str(train$kurtosis_picth_belt)
?read.csv
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
str(train$kurtosis_picth_belt)
summary(train$kurtosis_picth_belt)
train <- train[ ,colSums(is.na(train)) != nrow(train)]
dim(train)
colSums(is.na(train$kurtosis_picth_belt))
colSums(is.na(data.frame(train$kurtosis_picth_belt, train$X)))
colSums(is.na(data.frame(train$kurtosis_picth_belt, train$X))) >= nrow(train)*0.6
train <- train[ ,colSums(is.na(train)) >= nrow(train)*0.6]
dim(train)
?nearZeroVar
col.NZV <- nearZeroVar(train, saveMetrics = TRUE)
names(col.NZV)
drop.columns <- c("X", "problem_id", names(col.NZV))
drop.columns
length(drop.columns)
train <- train[, !colnames(train) %in% drop.columns]
dim(train)
test <- test[ ,colSums(is.na(train)) != nrow(train)]
test <- test[ ,colSums(is.na(test)) != nrow(test)]
dim(test)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
test <- test[ ,colSums(is.na(test)) != nrow(test)]
train <- train[ ,colSums(is.na(train)) >= nrow(train)*0.6]
col.NZV <- nearZeroVar(train, saveMetrics = TRUE)
drop.columns <- c("X", "problem_id", names(col.NZV))
test <- test[ , !colnames(test) %in% drop.columns]
train <- train[, !colnames(train) %in% drop.columns]
dim(test)
dim(train)
train <- train[, names(test)]
test <- test[ , !colnames(test) %in% drop.columns]
train <- train[, names(test)]
names(test)
dim(train)
names(test)
train <- train[, names(test)]
train <- train[, colnames(train) %in% names(test)]
dim(train)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- train[ ,colSums(is.na(train)) >= nrow(train)*0.6]
test <- test[ ,colSums(is.na(test)) != nrow(test)]
col.NZV <- nearZeroVar(train, saveMetrics = TRUE)
drop.columns <- c("X", "problem_id", names(col.NZV))
test <- test[ , !colnames(test) %in% drop.columns]
dim(test)
names(test)
train <- train[, colnames(train) %in% names(test)]
dim(train)
colnames(train) %in% names(test)
names(train)
train <- train[ ,colSums(is.na(train)) >= nrow(train)*0.6]
names(train)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- train[ ,colSums(is.na(train)) >= nrow(train)*0.6]
test <- test[ ,colSums(is.na(test)) != nrow(test)]
# drop the columns near zero variance
col.NZV <- nearZeroVar(train, saveMetrics = TRUE)
drop.columns <- c("X", "problem_id", names(col.NZV))
test <- test[ , !colnames(test) %in% drop.columns]
names(test)
names(train)
colnames(train) %in% names(test)
colnames(test) %in% drop.columns
!colnames(test) %in% drop.columns
drop.columns <- c("X", "problem_id")
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
test <- test[ ,colSums(is.na(test)) != nrow(test)]
colnames(test) %in% drop.columns
colnames(train) %in% names(test)
names(test) %in% names(train)
class(names(train))
drop.columns %in% names(test)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
test.name <- names(test)
?rank
dim(test)
dim(train)
train.name <- names(train)
t <- data.frame(test.name, train.name)
head(t)
head(t,20)
test.name %in% train.name
tail(t)
train <- train[ ,colSums(is.na(train)) <= nrow(train)*0.6]
test <- test[ ,colSums(is.na(test)) != nrow(test)]
dim(train)
dim(test)
col.NZV <- nearZeroVar(train, saveMetrics = TRUE)
drop.columns <- c("X", "problem_id", names(col.NZV))
test <- test[ , !colnames(test) %in% drop.columns]
colnames(train) %in% names(test)
dim(test)
dim(train)
train <- train[, colnames(train) %in% names(test)]
dim(train)
di(test)
dim(test)
names(train)
classe <- train$classe
head(classe)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
# if the count of NAs in a column equals to the number of rows, it must be entirely NA
# here drop the columns in which NAs are greater than 60% # of rows
train <- train[ ,colSums(is.na(train)) <= nrow(train)*0.6]
test <- test[ ,colSums(is.na(test)) != nrow(test)]
# drop the columns near zero variance
col.NZV <- nearZeroVar(train, saveMetrics = TRUE)
drop.columns <- c("X", "problem_id", names(col.NZV))
test <- test[ , !colnames(test) %in% drop.columns]
classe <- train$classe
train <- train[, colnames(train) %in% names(test)]
train <- data.frame(train, classe)
dim(train)
library(caret)
set.seed(123)
inTrain <-createDataPartition(train$user_name, p=0.6, list = FALSE)
training <- train[inTrain, ]
validation <- train[-inTrain, ]
dim(training)
dim(validation)
tree <- train(classe ~., data = training, method = "rpart")
tree$finalModel
t <- predict(tree, newdata = validation)
table(validation$classe, t)
confusionMatrix(validation$classe, t)
?raprt
?rpart
tree <- rpart(classe ~., data = training, method = "class")
tree <- rpart(classe ~., data = training, method = "class")
tree$finalModel
rattle::fancyRpartPlot(tree$finalModel)
dim(training)
tree <- rpart(classe ~., data = training, method = "class")
tree
rattle::fancyRpartPlot(tree)
t <- predict(tree, newdata = validation)
table(validation$classe, t)
length(t)
?predict
?predict.rpart
t <- predict(tree, newdata = validation, type = "class")
table(validation$classe, t)
confusionMatrix(validation$classe, t)
rf <- randomForest(class ~., data = training)
set.seed(1234)
rf <- randomForest(class ~., data = training)
library(randomForest)
rf <- randomForest(class ~., data = training)
?randomForest
rf <- randomForest(class ~., data = training, ntree = 50)
dim(training)
rf <- randomForest(class ~., data = training, ntree = 50)
rf <- randomForest(class ~., data = training, ntree = 50)
tree <- rpart(classe ~., data = training, method = "class")
rattle::fancyRpartPlot(tree)
library(randomForest)
set.seed(1234)
rf <- randomForest(class ~., data = training, ntree = 50)
names(training)
rf <- randomForest(classe ~., data = training, ntree = 50)
rf.predict <- predict(rd, newdata = validation)
rf.predict <- predict(rf, newdata = validation)
?predict.randomForest
head(rf.predict)
table(validation$classe, rf.predict)
confusionMatrix(validation$classe, rf.predict)
test.predicted <- predict(rf, newdata = test)
dim(test)
dim(validation)
test.predicted <- predict(rf, newdata = test)
test <- data.frame(test, classe)
test$classe <- NA
test.predicted <- predict(rf, newdata = test)
dim(test)
names(test)
names(validation)
test$classe <- head(classe,20)
test.predicted <- predict(rf, newdata = test)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
test <- test[ ,colSums(is.na(test)) != nrow(test)]
dim(test)
test <- test[ , !colnames(test) %in% drop.columns]
dim(test)
test.predicted <- predict(rf, newdata = test)
dim(validation)
predict(tree, newdata = test, type = "class")
predict(rf, newdata = validation, type = "class")
predict(tree, newdata = test, type = "class")
predict(rf, newdata = test, type = "class")
?predict.randomForest
test.predicted <- predict(rf, newdata = test, type = "response")
rf.predict <- predict(rf, newdata = validation)
table(validation$classe, rf.predict)
confusionMatrix(validation$classe, rf.predict)
test.predicted <- predict(rf, newdata = test)
test.predicted <- predict(rf, newdata = valiation[1:20,])
test.predicted <- predict(rf, newdata = validation[1:20,])
dim(validation)
dim(test)
class(validation)
class(test)
test$classe <- head(classe,20)
test.predicted <- predict(rf, newdata = test)
test <- data.frame(test, classe[1:20])
test.predicted <- predict(rf, newdata = test)
names(test)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
test <- test[ ,colSums(is.na(test)) != nrow(test)]
test <- test[ , !colnames(test) %in% drop.columns]
dim(test)
classe <- classe[1:20]
test <- data.frame(test, classe)
test.predicted <- predict(rf, newdata = test)
test.predicted <- predict(rf, newdata = validation)
test.predicted <- predict(rf, newdata = test, type = "class")
str(test)
str(train)
?levels
leves(train)
levels(train)
str(test)
levels(test$cvtd_timestamp) <- levels(train$cvtd_timestamp)
levels(test$new_window) <- levels(train$new_window)
test.predicted <- predict(rf, newdata = test, type = "class")
test.predicted
dir <- "D:/Coursera/8. Practical Machine Learning/Project"
setwd(dir)
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
# if the count of NAs in a column equals to the number of rows, it must be entirely NA
# here drop the columns in which NAs are greater than 60% # of rows
train <- train[ ,colSums(is.na(train)) <= nrow(train)*0.6]
test <- test[ ,colSums(is.na(test)) != nrow(test)]
# drop the columns near zero variance
col.NZV <- nearZeroVar(train, saveMetrics = TRUE)
drop.columns <- c("X", "problem_id", names(col.NZV))
test <- test[ , !colnames(test) %in% drop.columns]
classe <- train$classe
train <- train[, colnames(train) %in% names(test)]
train <- data.frame(train, classe)
library(caret)
set.seed(123)
inTrain <-createDataPartition(train$user_name, p=0.6, list = FALSE)
training <- train[inTrain, ]
validation <- train[-inTrain, ]
tree <- train(classe ~., data = training, method = "rpart")
rattle::fancyRpartPlot(tree)
tree.predict <- predict(tree, newdata = validation, type = "class")
confusionMatrix(validation$classe, tree.predict)
tree.predict <- predict(tree, newdata = validation, type = "class")
?rpart
caret
?caret
??caret
tree <- rpart(classe ~., data = training, method = "class")
rattle::fancyRpartPlot(tree)
table(validation$classe, tree.predict)
tree.predict <- predict(tree, newdata = validation, type = "class")
table(validation$classe, tree.predict)
confusionMatrix(validation$classe, tree.predict)
library(randomForest)
set.seed(1234)
rf <- randomForest(classe ~., data = training, ntree = 50)
rf.predict <- predict(rf, newdata = validation)
table(validation$classe, rf.predict)
confusionMatrix(validation$classe, rf.predict)
plot(rf)
?plot.randomForest
rf <- randomForest(classe ~., data = training, ntree = 200)
plot(rf, main = "MSE versus number of tree of Random Forest")
?gbm
library(gbm)
set.seed(12345)
?gbm
?nearZeroVar
boost <- gbm(classe ~., data = training, n.trees = 200)
boost
boost.predict <- predict(boost, newdata = validation)
boost.predict <- predict(boost, newdata = validation, n.trees = 200)
confusionMatrix(validation$classe, boost.predict)
head(boost.predict)
?gbm
summary(boost)
summary(boost.predict)
?predict.gbm
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "response")
confusionMatrix(validation$classe, boost.predict)
head(boost.predict)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "link")
head(boost.predict)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "link")
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200)
confusionMatrix(validation$classe, boost.predict)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "link")
confusionMatrix(validation$classe, boost.predict)
display(boosting)
summary(boost)
importantce(boost)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200, verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "response")
confusionMatrix(validation$classe, boost.predict)
summary(boost.predict)
head(boost.predict)
str(as.factor(boost.predict))
boost <- train(classe ~., data = training, method = "glm", verbose = FALSE)
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
boost <- train(classe ~., data = training, method = "glm", verbose = FALSE, trControl = fitControl)
boost <- train(classe ~., data = training, method = "gbm", verbose = FALSE, trControl = fitControl)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200, verbose = FALSE)
summary(boost)
names(boost)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "response")
head(boost.predict)
summary(as.factor(boost.predict))
str(as.factor(boost.predict))
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200, verbose = FALSE,
trCOntrol = fitControl)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200, verbose = FALSE,
trControl = fitControl)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200, verbose = FALSE)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200, verbose = FALSE)
?predict.gbm
dim(boost.predict)
head(boost.predict)
boost.predict[1,1]
class(boost.predict)
boost.predict[1:6,]
boost.predict[1:6, , ]
boost.predict[1:6,1,1
]
boost.predict[1:6,1,]
boost.predict[1:6, ,]
?apply
boost.predClass <- apply(boost.predict, MARGIN = 1, which.max)
confusionMatrix(validation$classe, boost.predClass)
head(boost.predClass)
class(boost.predClass)
colnames(boost.predict)
boost.predClass <- apply(boost.predict, MARGIN = 1, colnames(boost.predict)[which.max] )
which.max(boost.predict[1,,])
a <- which.max(boost.predict[1,,])
a
class(a)
names(a)
boost.predClass <- apply(boost.predict, MARGIN = 1, names(which.max) )
names(boost.predict)[a]
names(boost.predict)[1]
names(boost.predict)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "response")
names(boost.predict)
boost.predict[1:6,,]
names(boost.predict[,,])
colnames(boost.predict)
boost.predClass <- colnames(boost.predict)[boost.predClass]
head(boost.predClass)
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max)
boost.predClass <- colnames(boost.predict)[boost.predIndex]
confusionMatrix(validation$classe, boost.predClass)
str(boost.predClass)
confusionMatrix(validation$classe, as.factor(boost.predClass))
levels(boost.predClass) <- levels(validation$classe)
confusionMatrix(validation$classe, boost.predClass)
str(boost.predClass)
str(validation$classe)
boost.predClass <- as.factor( colnames(boost.predict)[boost.predIndex] )
str(boost.predClass)
levels(boost.predClass) <- levels(validation$classe)
str(boost.predClass)
confusionMatrix(validation$classe, boost.predClass)
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max)
summary(boost.predIndex)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max)
boost.predClass <- as.factor( colnames(boost.predict)[boost.predIndex] )
levels(boost.predClass) <- levels(validation$classe)
confusionMatrix(validation$classe, boost.predClass)
summary(boost.predClass)
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max)
boost.predClass <- as.factor( colnames(boost.predict)[boost.predIndex] )
summary(boost.predIndex)
summary(boost.predClass)
boost.predClass <- colnames(boost.predict)[boost.predIndex]
summary(boost.predClass)
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max)
boost.predClass <- colnames(boost.predict)[boost.predIndex]
summary(boost.predClass)
head(boost.predClass)
str(boost.predClass)
str(as.factor(boost.predClass))
summary(boost.predIndex)
plot(boost.predIndex)
boost.predict <- predict(boost, newdata = validation, type = "response")
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 1000, verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 1000, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max)
boost.predClass <- colnames(boost.predict)[boost.predIndex]
levels(boost.predClass) <- levels(validation$classe)
confusionMatrix(validation$classe, boost.predClass)
boost.predict <- predict(boost, newdata = validation, n.trees = 3000, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max)
boost.predClass <- colnames(boost.predict)[boost.predIndex]
levels(boost.predClass) <- levels(validation$classe)
confusionMatrix(validation$classe, boost.predClass)
plot(boost.predClass)
plot(boost.predIndex)
plot(boost.predict)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 5000, verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 5000, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 1000,
shrinkage = 0.2, verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 1000, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 100,
shrinkage = 0.2, verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 100, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 200,
shrinkage = 0.2, verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 200, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 100,
shrinkage = 0.2, verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 100, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
plot(boost.predict)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 100,
shrinkage = 0.2, interaction.depth = 3,verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 100, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
names(boost.predict)
str(boost.predict)
str(boot)
str(boost)
names(boost)
boost$interaction.depth
boost$cv.folds
set.seed(12345)
# fitControl <- trainControl(method = "repeatedcv",
#                            number = 5,
#                            repeats = 1)
# boost <- train(classe ~., data = training, method = "gbm", verbose = FALSE, trControl = fitControl)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 100,
shrinkage = 0.2, interaction.depth = 3,verbose = FALSE)
# notice that boost.predict is a array
boost.predict <- predict(boost, newdata = validation, n.trees = 100, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
set.seed(12345)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 100,
shrinkage = 0.2, interaction.depth = 3,verbose = FALSE)
boost.predict <- predict(boost, newdata = validation, n.trees = 100, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
levels(test$cvtd_timestamp) <- levels(train$cvtd_timestamp)
levels(test$new_window) <- levels(train$new_window)
test.predicted <- predict(rf, newdata = test, type = "class")
test.predicted
levels(test$cvtd_timestamp) <- levels(train$cvtd_timestamp)
