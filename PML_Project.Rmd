---
title: "Practical Machine Learning Project_Coursera"
author: "Xiang Cao"
date: "February 14, 2016"
output: html_document
---

##1. Loading data and data cleaning
```{r}
test <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
train <- train[ ,colSums(is.na(train)) <= nrow(train)*0.6]
test <- test[ ,colSums(is.na(test)) != nrow(test)]
```

If the count of NAs in a column equals to the number of rows, then the column must be entirely NA.

```{r}
library(caret)
col.NZV <- nearZeroVar(train, saveMetrics = TRUE) # Figure out the columns that has near zero variance
drop.columns <- c("X", "problem_id", names(col.NZV))
test <- test[ , !colnames(test) %in% drop.columns]
classe <- train$classe
train <- train[, colnames(train) %in% names(test)]
train <- data.frame(train, classe)
```

Drop the unnecessary columns, including $observation id$, $problem id$, and $near zero variance$. Since variables in $test$ data set are less than $train$ data set, we use the variables appear in both data sets to fit the model.

```{r}
set.seed(123)
inTrain <-createDataPartition(train$user_name, p=0.6, list = FALSE) 
training <- train[inTrain, ]
validation <- train[-inTrain, ]
```

Creat new $training$ data set and $validation$ data set from the original data.

##2. Model fitting: Decision Tree
```{r}
library(rpart)
# tree <- train(classe ~., data = training, method = "rpart") # it seems there is error using caret?
tree <- rpart(classe ~., data = training, method = "class")
rattle::fancyRpartPlot(tree)
tree.predict <- predict(tree, newdata = validation, type = "class")
confusionMatrix(validation$classe, tree.predict)
```

The Decision tree performs quite well. Accuracy is $0.8725$

##3. Model fitting: Random Forest
```{r}
library(randomForest)
set.seed(1234)
rf <- randomForest(classe ~., data = training, ntree = 200)
rf.predict <- predict(rf, newdata = validation)
confusionMatrix(validation$classe, rf.predict)
plot(rf, main = "MSE versus number of tree of Random Forest")
```

Random forest performs much better than decision tree. The accuracy attains $0.9989$. The the plot we can see only after fitting 30 trees, RandomForst reaches a very small error rate. The default number of tree (ntree) is 500, however we only need set a small number to reduce computing time.

##4. Model fitting: Boosting
```{r}
library(gbm)
set.seed(12345)
boost <- gbm(classe ~., data = training, distribution = "multinomial", n.trees = 100, 
             shrinkage = 0.2, interaction.depth = 3,verbose = FALSE)
# notice that boost.predict is a array
boost.predict <- predict(boost, newdata = validation, n.trees = 100, type = "response")
boost.predict[1:6, ,]
boost.predIndex <- apply(boost.predict, MARGIN = 1, which.max) # find out the index of maximal prob. each row
boost.predClass <- colnames(boost.predict)[boost.predIndex] # convert index into class A-E
levels(boost.predClass) <- levels(validation$classe) # make sure they have the same level
confusionMatrix(validation$classe, boost.predClass)
```

Even though the overall accuracy is $0.9964$, actually Boosting performs a little bit better than Random forest if we re-fit the model with other parameters. When fitting Boosting, you need tune parameters carefully including $n.trees$, $shrinkage$, $interaction.depth$, etc. The running time increases a lot if you increase $n.trees$ and $interaction.depth$.

##5. Predicting 20 test cases.
```{r}
levels(test$cvtd_timestamp) <- levels(train$cvtd_timestamp)
levels(test$new_window) <- levels(train$new_window)
test.predicted <- predict(rf, newdata = test, type = "class")
test.predicted
```

Consider both accuracy and running time, we choose $RandomForest$ as the final model. Please note that since the test data set are too small, we need manually assign the levels of specific columns in $train$ dataset to corresponding columns in $test$ dataset, so that test dataset could be predicted. Another method is that you can combine the training and testing dataset at the very begining and then split them, which would make them have the same levels.